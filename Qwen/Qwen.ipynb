{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Qwen2Config 配置类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2Config:\n",
    "    \"\"\"\n",
    "    配置类，用于存储并传递模型超参数。\n",
    "    可根据需要自行修改默认值。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        max_position_embeddings=2048,\n",
    "        rope_theta=10000.0,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_act=\"silu\",          # MLP激活函数\n",
    "        attention_bias=False,       # 是否在Q,K,V投影时使用bias\n",
    "        rms_norm_eps=1e-6,\n",
    "        pad_token_id=0,\n",
    "        num_key_value_heads=4,      # GQA相关(可改)\n",
    "        _attn_implementation=\"eager\"  # 注意力实现方式: eager, flash_attention_2, sdpa\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.rope_theta = rope_theta\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.hidden_act = hidden_act\n",
    "        self.attention_bias = attention_bias\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self._attn_implementation = _attn_implementation\n",
    "        \n",
    "        # 一些后续可能用到的属性\n",
    "        self.gradient_checkpointing = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 预训练模型基类 (简化版)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2PreTrainedModel(nn.Module):\n",
    "    \"\"\"\n",
    "    一个简化的预训练模型基类，仅做示例。\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Qwen2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        简化的权重初始化逻辑，也可使用 xavier_uniform、kaiming_uniform 等更复杂初始化。\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def _backward_compatibility_gradient_checkpointing(self):\n",
    "        \"\"\"\n",
    "        兼容一些老版本或者transformers内部的梯度检查点设定。\n",
    "        \"\"\"\n",
    "        self.gradient_checkpointing = self.config.gradient_checkpointing\n",
    "\n",
    "    def post_init(self):\n",
    "        \"\"\"\n",
    "        初始化结束后的函数，一般用于权重初始化和其他兼容性检查。\n",
    "        \"\"\"\n",
    "        self.init_weights()\n",
    "        self._backward_compatibility_gradient_checkpointing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Qwen2模型主体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2Model(Qwen2PreTrainedModel):\n",
    "    def __init__(self, config: Qwen2Config):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # 词向量Embedding\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        \n",
    "        # Decoder层，存 num_hidden_layers 个\n",
    "        self.layers = nn.ModuleList(\n",
    "            [Qwen2DecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        # 输出的层归一化\n",
    "        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        \n",
    "        # 是否启用gradient_checkpointing(节省显存但会重复计算)\n",
    "        self.gradient_checkpointing = config.gradient_checkpointing\n",
    "\n",
    "        # 执行权重初始化以及兼容性检查\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "        use_cache=False,\n",
    "        past_key_value=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        ----------\n",
    "        参数:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, 1, seq_len, seq_len]，下三角或其他mask\n",
    "            position_ids: 如果不传，会默认按顺序 0,1,2...\n",
    "            output_hidden_states: 是否输出每层的hidden_states\n",
    "            output_attentions: 是否输出每层的attention_weights\n",
    "            use_cache: 推理阶段常用，是否使用并返回 kv_cache\n",
    "            past_key_value: 前面步骤缓存的 kv\n",
    "        \"\"\"\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"Please provide input_ids\")\n",
    "\n",
    "        # 准备保存\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        \n",
    "        # token -> Emb\n",
    "        inputs_embeds = self.embed_tokens(input_ids)   # (bsz, seq_len, hidden_size)\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # 进入decoder层\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=None if past_key_value is None else past_key_value[idx],\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            # 如果 use_cache=True，会返回 present_key_value\n",
    "            if use_cache:\n",
    "                # 第2个就是 present_key_value\n",
    "                present_key_value = layer_outputs[2]\n",
    "        \n",
    "        # norm层\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # 如果需要输出每层hs\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        # 返回值可自行封装，这里做简单返回\n",
    "        outputs = (hidden_states,)\n",
    "        if output_hidden_states:\n",
    "            outputs += (all_hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs += (all_attentions,)\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Qwen2DecoderLayer 解码层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = QWEN2_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n",
    "        self.mlp = Qwen2MLP(config)\n",
    "        # 两个RMSNorm\n",
    "        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        use_cache=False\n",
    "    ):\n",
    "        # 1) 自注意力\n",
    "        residual = hidden_states\n",
    "        # RMSNorm\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        attn_output, attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        # 2) MLP\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 注意力实现: Qwen2Attention (eager 版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将KV进行GQA扩展: hidden_states形状为 [bs, num_key_value_heads, seq_len, head_dim]\n",
    "    需要扩展为 [bs, num_key_value_heads * n_rep, seq_len, head_dim]\n",
    "    \"\"\"\n",
    "    batch_size, num_heads_kv, seq_len, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    # 用expand+reshape方式，而不是repeat，可以更省内存\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(\n",
    "        batch_size, num_heads_kv, n_rep, seq_len, head_dim\n",
    "    )\n",
    "    return hidden_states.reshape(batch_size, num_heads_kv * n_rep, seq_len, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    对张量最后一维前后两半部分进行旋转拼接:\n",
    "    x[..., : mid] -> -x[..., mid:]\n",
    "    x[..., mid:] -> x[..., : mid]\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    \"\"\"\n",
    "    q,k: [batch_size, num_heads, seq_len, head_dim]\n",
    "    position_ids: [batch_size, seq_len]\n",
    "    cos,sin: [max_seq_len, head_dim]\n",
    "    \"\"\"\n",
    "    # -> [batch_size, seq_len, head_dim]\n",
    "    cos = cos[position_ids]\n",
    "    sin = sin[position_ids]\n",
    "\n",
    "    # 在 num_heads 位置上插一个维度 => [batch_size, 1, seq_len, head_dim]\n",
    "    cos = cos.unsqueeze(1)\n",
    "    sin = sin.unsqueeze(1)\n",
    "\n",
    "    # 之后和 q,k 相乘 => [batch_size, num_heads, seq_len, head_dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    RoPE的旋转位置编码实现。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000.0,\n",
    "        device=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "\n",
    "        # 计算 inv_freq: [dim//2]\n",
    "        # 形如 1/(base^(2i/dim))\n",
    "        inv_freq = 1.0 / (\n",
    "            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32).float() / self.dim)\n",
    "        )\n",
    "        if device is not None:\n",
    "            inv_freq = inv_freq.to(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # 提前构建cos,sin缓存\n",
    "        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.float32)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=dtype)\n",
    "        t = t.unsqueeze(-1)  # shape: (seq_len, 1)\n",
    "\n",
    "        # outer product: (seq_len, dim//2)\n",
    "        freqs = t * self.inv_freq\n",
    "        # cos,sin shape: (seq_len, dim//2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)  # 在dim=-1拼接\n",
    "\n",
    "        # 注册缓存\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_heads, seq_len, head_dim]\n",
    "        seq_len: 当前需要位置编码的长度\n",
    "        \"\"\"\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[2]\n",
    "\n",
    "        # 如果 seq_len 超过当前缓存，则重新初始化\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(x.dtype),\n",
    "            self.sin_cached[:seq_len].to(x.dtype),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self Attention, 采用RoPE位置编码, 以及GQA的可选实现。\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True  # 一般decoder是causal\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size必须能被num_heads整除，\"\n",
    "                f\"当前 hidden_size={self.hidden_size}, num_heads={self.num_heads}\"\n",
    "            )\n",
    "        \n",
    "        # Q,K,V投影层\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        # out投影层\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = Qwen2RotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        # hidden_states: [batch_size, seq_len, hidden_size]\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # Q,K,V\n",
    "        query_states = self.q_proj(hidden_states)  # [bsz, seq_len, num_heads*head_dim]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # 多头展开\n",
    "        # Q: [bsz, num_heads, seq_len, head_dim]\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # K,V: [bsz, num_key_value_heads, seq_len, head_dim]\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # RoPE\n",
    "        kv_seq_len = q_len  # 这里假设不需要past_kv\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "        if position_ids is None:\n",
    "            bsz, seq_len = input_ids.shape\n",
    "            position_ids = (\n",
    "                torch.arange(seq_len, device=input_ids.device)\n",
    "                .unsqueeze(0)         # [1, seq_len]\n",
    "                .expand(bsz, -1)      # [bsz, seq_len]\n",
    "            )\n",
    "        q_embed, k_embed = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "        query_states, key_states = q_embed, k_embed\n",
    "\n",
    "        # GQA, 重复KV\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)    # (bsz, num_heads, seq_len, head_dim)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        # 计算注意力得分\n",
    "        # [bsz, num_heads, q_len, k_len]\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # 加上mask(比如causal mask)\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        # 与values相乘\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "        # 形状还原\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, self.num_heads * self.head_dim)\n",
    "        # out投影\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # present_key_value在推理时会用到\n",
    "        present_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        outputs = (attn_output, attn_weights if output_attentions else None, present_key_value)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Qwen2MLP (MLP部分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT2FN = {\n",
    "    \"silu\": torch.nn.functional.silu,\n",
    "    \"relu\": torch.nn.functional.relu,\n",
    "    \"gelu\": torch.nn.functional.gelu,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2MLP(nn.Module):\n",
    "    def __init__(self, config: Qwen2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "\n",
    "        # 两个并行投影: gate_proj / up_proj\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        # 再一个下投影\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 类似SwiGLU: (gate激活) * (up线性) -> 下投影\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Qwen2RMSNorm (RMSNorm实现)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        RMSNorm实现\n",
    "        公式: x * weight / sqrt(mean(x^2) + eps)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # 先转fp32计算，再转回原dtype\n",
    "        variance = hidden_states.float().pow(2).mean(-1, keepdim=True)\n",
    "        normed = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return (self.weight * normed).to(input_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) 额外：登记Attention实现类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2FlashAttention2(Qwen2Attention):\n",
    "    \"\"\"\n",
    "    此处仅示例，如果需要可结合FlashAttention2实现，暂留空。\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
    "        super().__init__(config, layer_idx)\n",
    "\n",
    "class Qwen2SdpaAttention(Qwen2Attention):\n",
    "    \"\"\"\n",
    "    此处仅示例，如果需要可结合torch.sdpa计算，暂留空。\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
    "        super().__init__(config, layer_idx)\n",
    "\n",
    "QWEN2_ATTENTION_CLASSES = {\n",
    "    \"eager\": Qwen2Attention,  # 这里就是本例子中常用的\n",
    "    \"flash_attention_2\": Qwen2FlashAttention2,\n",
    "    \"sdpa\": Qwen2SdpaAttention,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) 演示：随机输入，跑一遍模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden_states shape: torch.Size([2, 10, 256])\n",
      "number of hidden_states (including embedding): 5\n",
      "  hidden_states[0].shape = torch.Size([2, 10, 256])\n",
      "  hidden_states[1].shape = torch.Size([2, 10, 256])\n",
      "  hidden_states[2].shape = torch.Size([2, 10, 256])\n",
      "  hidden_states[3].shape = torch.Size([2, 10, 256])\n",
      "  hidden_states[4].shape = torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1) 创建配置\n",
    "    config = Qwen2Config(\n",
    "        vocab_size=1000,\n",
    "        hidden_size=256,\n",
    "        num_hidden_layers=4,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=1024,\n",
    "        max_position_embeddings=128,\n",
    "        rope_theta=10000.0,\n",
    "        attention_dropout=0.1,\n",
    "        hidden_act=\"silu\",\n",
    "        attention_bias=True,\n",
    "        rms_norm_eps=1e-6,\n",
    "        pad_token_id=0,\n",
    "        num_key_value_heads=2,   # 测试GQA，实际中可改\n",
    "        _attn_implementation=\"eager\"\n",
    "    )\n",
    "\n",
    "    # 2) 实例化模型\n",
    "    model = Qwen2Model(config)\n",
    "\n",
    "    # 3) 准备随机输入：batch=2，seq_len=10\n",
    "    input_ids = torch.randint(0, config.vocab_size, (2, 10))\n",
    "\n",
    "    # 构造一个简单的下三角mask (causal mask)\n",
    "    seq_len = input_ids.size(1)\n",
    "    causal_mask = torch.full((seq_len, seq_len), float(\"-inf\"))\n",
    "    causal_mask = torch.triu(causal_mask, diagonal=1)  # 上三角置-inf\n",
    "    # 扩展到 [batch_size, num_heads, seq_len, seq_len]\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # 4) 前向传播\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=causal_mask,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    # 5) 查看输出\n",
    "    print(\"last hidden_states shape:\", outputs[0].shape)\n",
    "    if len(outputs) > 1:\n",
    "        # all_hidden_states\n",
    "        print(\"number of hidden_states (including embedding):\", len(outputs[1]))\n",
    "        for idx, hs in enumerate(outputs[1]):\n",
    "            print(f\"  hidden_states[{idx}].shape = {hs.shape}\")\n",
    "    \n",
    "    # 也可以根据需要，对 outputs 做进一步处理(如再加一个Linear输出logits等)。\n",
    "    # 由于是简化演示，这里仅打印形状。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
