# 1.Qwen

## 1.1 ä»‹ç»

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆllmï¼‰å·²ç»å½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä½¿ä»¥å‰è¢«è®¤ä¸ºæ˜¯äººç±»ç‹¬æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æˆä¸ºå¯èƒ½ã€‚ 

Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰æ˜¯é˜¿é‡Œå·´å·´å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚ 

QwenåŒ…å«å…·æœ‰ä¸åŒå‚æ•°è®¡æ•°çš„ä¸åŒæ¨¡å‹ã€‚å®ƒåŒ…æ‹¬Qwenï¼ˆåŸºç¡€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰å’ŒQwen- chatï¼ˆä½¿ç”¨äººç±»å¯¹é½æŠ€æœ¯è¿›è¡Œå¾®è°ƒçš„èŠå¤©æ¨¡å‹ï¼‰ã€‚åŸºæœ¬è¯­è¨€æ¨¡å‹åœ¨ä¼—å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè€ŒèŠå¤©æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è®­ç»ƒçš„æ¨¡å‹ï¼Œå…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚èŠå¤©æ¨¡å‹å…·æœ‰ç”¨äºåˆ›å»ºä»£ç†åº”ç”¨ç¨‹åºçš„é«˜çº§å·¥å…·ä½¿ç”¨å’Œè§„åˆ’åŠŸèƒ½ï¼Œå³ä½¿ä¸ä½¿ç”¨ä»£ç è§£é‡Šå™¨ç­‰å¤æ‚ä»»åŠ¡çš„å¤§å‹æ¨¡å‹ç›¸æ¯”ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒQwenæœ‰ä¸“é—¨çš„ç¼–ç æ¨¡å‹Code-Qwenå’ŒCode-Qwen- chatï¼Œä»¥åŠåŸºäºåŸºæœ¬è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨¡å‹Math-Qwen-Chatã€‚ä¸å¼€æºæ¨¡å‹ç›¸æ¯”ï¼Œè¿™äº›æ¨¡å‹è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶ä¸”ç•¥å¾®è½åäºä¸“æœ‰æ¨¡å‹\cite{bai2023qwen}ã€‚

Qwen - 2æ˜¯ä¸‹ä¸€ä»£ï¼Œä»…è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œå…·æœ‰ï¼šRMSNormå½’ä¸€åŒ–å±‚ï¼Œç”¨äºæ”¹è¿›ä½ç½®ç¼–ç çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œå…·æœ‰å…¸å‹å˜å‹å™¨æ®‹ä½™è·¯å¾„çš„å¤šå¤´è‡ªæ³¨æ„ï¼Œç”¨äºæ›´ä¸°å¯Œè½¬æ¢çš„å‰é¦ˆï¼ˆMLPï¼‰å­å±‚ï¼Œç”¨äºè¯­è¨€å»ºæ¨¡è¾“å‡ºçš„æœ€ç»ˆçº¿æ€§æŠ•å½±ã€‚

å®ƒçš„å·¥ä½œæµç¨‹å¾ˆç®€å•ï¼šå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼ŒåµŒå…¥æ ‡è®°ï¼Œä½¿ç”¨RMSNorm +æ³¨æ„åŠ›+ MLPå—åœ¨å¤šä¸ªTransformerè§£ç å™¨å±‚ä¸­è¿è¡Œå®ƒä»¬ï¼Œæœ€åç”Ÿæˆæ ‡è®°logitsã€‚é€šè¿‡å †å è¿™äº›å±‚å¹¶ä½¿ç”¨å‰©ä½™è¿æ¥ï¼ŒQwenâ€‘2å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶å¯ä»¥é€‚åº”å„ç§è¯­è¨€ä»»åŠ¡ã€‚Qwen2æ¡†æ¶å¦‚å›¾1æ‰€ç¤ºã€‚

![Framework](D:\Document\GitHub\White-box\README.assets\189cbca174d58f1e50aebcf9cbb2e6e3e2a48d7e.jpg)

## 1.2 ä¸Qwenç›¸å…³çš„50ä¸ªä¸“ä¸šæœ¯è¯­

æ¶µç›–å…¶æ¶æ„ã€è®­ç»ƒã€æ¨ç†ã€ä¼˜åŒ–ç­‰æ–¹é¢

### **1-10: åŸºç¡€æ¦‚å¿µ**

1. **LLM (Large Language Model)** - å¤§å‹è¯­è¨€æ¨¡å‹
2. **Transformer** - å˜æ¢å™¨æ¶æ„ï¼ŒLLMçš„æ ¸å¿ƒ
3. **MoE (Mixture of Experts)** - ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œæé«˜è®¡ç®—æ•ˆç‡
4. **Decoder-only Model** - ä»…åŒ…å«è§£ç å™¨çš„Transformeræ¶æ„
5. **Self-Attention** - è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerçš„æ ¸å¿ƒç»„ä»¶
6. **Multi-Head Attention** - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜è¡¨ç¤ºèƒ½åŠ›
7. **FFN (Feed Forward Network)** - å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒTransformerä¸­çš„MLPæ¨¡å—
8. **Positional Encoding** - ä½ç½®ç¼–ç ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£åºåˆ—ä¿¡æ¯
9. **Tokenization** - åˆ†è¯ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„Tokenåºåˆ—
10. **Subword Tokenization** - å­è¯åˆ†è¯ï¼Œå¦‚BPEã€WordPiece

### **11-20: è®­ç»ƒç›¸å…³**

1. **Pre-training** - é¢„è®­ç»ƒï¼Œå¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ é˜¶æ®µ
2. **Fine-tuning** - å¾®è°ƒï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è°ƒæ•´æ¨¡å‹å‚æ•°
3. **RLHF (Reinforcement Learning from Human Feedback)** - åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ 
4. **LoRA (Low-Rank Adaptation)** - ä½ç§©é€‚é…ï¼Œé™ä½å¾®è°ƒæˆæœ¬
5. **PEFT (Parameter-Efficient Fine-Tuning)** - å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•
6. **Gradient Checkpointing** - æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
7. **FP16 (Half-Precision Floating Point)** - åŠç²¾åº¦æµ®ç‚¹è¿ç®—ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
8. **BF16 (Brain Floating Point)** - Googleå¼€å‘çš„16ä½æµ®ç‚¹æ ¼å¼ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
9. **Zero Redundancy Optimizer (ZeRO)** - åˆ†å¸ƒå¼ä¼˜åŒ–ç­–ç•¥ï¼Œé™ä½æ˜¾å­˜å¼€é”€
10. **Distributed Training** - åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŠ é€Ÿå¤§æ¨¡å‹è®­ç»ƒ

### **21-30: æ¨ç†ä¸ä¼˜åŒ–**

1. **Inference** - æ¨ç†ï¼Œå³æ¨¡å‹åœ¨è®­ç»ƒåæ‰§è¡Œä»»åŠ¡çš„è¿‡ç¨‹
2. **Quantization** - é‡åŒ–ï¼Œå‡å°‘æ¨¡å‹å¤§å°å¹¶æé«˜æ¨ç†é€Ÿåº¦
3. **4-bit Quantization** - 4æ¯”ç‰¹é‡åŒ–ï¼Œæç«¯å‹ç¼©æ¨¡å‹
4. **KV Cache (Key-Value Cache)** - å…³é”®-å€¼ç¼“å­˜ï¼ŒåŠ é€Ÿæ¨ç†
5. **Speculative Decoding** - é¢„æµ‹å¼è§£ç ï¼Œæé«˜æ¨ç†é€Ÿåº¦
6. **Beam Search** - æŸæœç´¢ï¼Œæå‡ç”Ÿæˆæ–‡æœ¬è´¨é‡
7. **Top-k Sampling** - é™å®šæœ€é«˜æ¦‚ç‡çš„kä¸ªè¯ï¼Œæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§
8. **Top-p Sampling (Nucleus Sampling)** - æ ¸é‡‡æ ·ï¼Œé™åˆ¶ç´¯è®¡æ¦‚ç‡på†…çš„è¯
9. **Temperature Scaling** - ç”Ÿæˆæ¸©åº¦æ§åˆ¶ï¼Œè°ƒæ•´æ¨¡å‹è¾“å‡ºçš„ç¡®å®šæ€§
10. **Greedy Decoding** - è´ªå¿ƒè§£ç ï¼Œæ¯æ­¥é€‰æ‹©æœ€ä¼˜è¯ï¼Œä½†å¯èƒ½å±€éƒ¨æœ€ä¼˜

### **31-40: æ¨¡å‹æ¶æ„ä¸ä¼˜åŒ–**

1. **Sparse Attention** - ç¨€ç–æ³¨æ„åŠ›ï¼Œå‡å°‘è®¡ç®—é‡
2. **FlashAttention** - é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ï¼Œæé«˜æ¨ç†é€Ÿåº¦
3. **Rotary Positional Embedding (RoPE)** - æ—‹è½¬ä½ç½®ç¼–ç ï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›
4. **ALiBi (Attention Linear Bias)** - çº¿æ€§åç½®æ³¨æ„åŠ›ï¼Œå¢å¼ºé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›
5. **LayerNorm (Layer Normalization)** - å±‚å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
6. **Pre-LN (Pre-Normalization)** - å½’ä¸€åŒ–æå‰ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
7. **Residual Connection** - æ®‹å·®è¿æ¥ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
8. **Mixture-of-Depths (MoD)** - æ·±åº¦æ··åˆæœºåˆ¶ï¼Œæé«˜è®¡ç®—æ•ˆç‡
9. **Sparse MoE** - ç¨€ç–MoEï¼Œä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ç½‘ç»œï¼Œé™ä½è®¡ç®—æˆæœ¬
10. **Dense MoE** - å¯†é›†MoEï¼Œæ‰€æœ‰ä¸“å®¶ç½‘ç»œéƒ½å‚ä¸è®¡ç®—

### **41-50: åº”ç”¨ä¸ç”Ÿæ€**

1. **Chatbot** - èŠå¤©æœºå™¨äººï¼ŒQwençš„ä¸»è¦åº”ç”¨åœºæ™¯
2. **RAG (Retrieval-Augmented Generation)** - æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæé«˜äº‹å®æ€§
3. **Few-shot Learning** - å°æ ·æœ¬å­¦ä¹ ï¼Œæ¨¡å‹é€šè¿‡å°‘é‡ç¤ºä¾‹å®Œæˆæ–°ä»»åŠ¡
4. **Zero-shot Learning** - é›¶æ ·æœ¬å­¦ä¹ ï¼Œæ¨¡å‹æ— éœ€ç¤ºä¾‹å³å¯æ¨ç†
5. **Prompt Engineering** - æç¤ºè¯å·¥ç¨‹ï¼Œä¼˜åŒ–è¾“å…¥æç¤ºä»¥æ”¹å–„è¾“å‡º
6. **Chain-of-Thought (CoT)** - æ€ç»´é“¾æç¤ºï¼Œå¼•å¯¼æ¨¡å‹æ¨ç†å¤šæ­¥é—®é¢˜
7. **Function Calling** - å‡½æ•°è°ƒç”¨ï¼Œç»“åˆå¤–éƒ¨å·¥å…·å¢å¼ºèƒ½åŠ›
8. **Multi-modal Learning** - å¤šæ¨¡æ€å­¦ä¹ ï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰æ•°æ®
9. **AGI (Artificial General Intelligence)** - é€šç”¨äººå·¥æ™ºèƒ½ï¼ŒQwenç­‰LLMçš„ç»ˆæç›®æ ‡
10. **Open-source LLM** - å¼€æºå¤§æ¨¡å‹ï¼Œå¦‚Qwen-7Bã€Qwen-14Bï¼Œä¾›ç ”ç©¶ä¸å¼€å‘ä½¿ç”¨

## 1.3 Mixture of Experts (MoE) ä¾‹å­è¯´æ˜

MoEï¼ˆMixture of Expertsï¼‰æ˜¯ä¸€ç§æ¨¡å‹æ¶æ„ï¼Œå®ƒé€šè¿‡å¤šä¸ªä¸“å®¶ï¼ˆExpertsï¼‰æ¨¡å‹ååŒå·¥ä½œï¼Œå¹¶ç”±ä¸€ä¸ªé—¨æ§ï¼ˆGatingï¼‰ç½‘ç»œæ¥å†³å®šä¸åŒè¾“å…¥æ•°æ®åº”ç”±å“ªäº›ä¸“å®¶æ¥å¤„ç†ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶é€‚ç”¨äºå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚

#### **ä¾‹å­1ï¼šæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„ MoE**

å‡è®¾æˆ‘ä»¬è¦è®­ç»ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹æ¥åˆ†ç±»æ–°é—»æ–‡ç« çš„ç±»åˆ«ï¼Œä¾‹å¦‚â€œä½“è‚²â€ã€â€œç§‘æŠ€â€ã€â€œæ”¿æ²»â€ã€â€œå¨±ä¹â€ç­‰ã€‚

- **ä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰ï¼š**
  æˆ‘ä»¬å¯ä»¥è®­ç»ƒå¤šä¸ªä¸åŒçš„ä¸“å®¶æ¨¡å‹ï¼Œæ¯ä¸ªä¸“å®¶å¯èƒ½ä¸“æ³¨äºä¸åŒçš„æ–‡æœ¬ç‰¹å¾ã€‚ä¾‹å¦‚ï¼š
  - **Expert 1**ï¼šä¸“æ³¨äºçŸ­æ–‡æœ¬ï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–çŸ­æ–‡æœ¬æ¨¡å¼ã€‚
  - **Expert 2**ï¼šä¸“æ³¨äºé•¿æ–‡æœ¬ï¼Œä½¿ç”¨Transformerï¼ˆå¦‚BERTï¼‰å¤„ç†é•¿æ–‡æœ¬ä¾èµ–ã€‚
  - **Expert 3**ï¼šä¸“æ³¨äºæ–°é—»é¢†åŸŸï¼Œå­¦ä¹ æ–°é—»æ–‡ç« çš„ç‰¹å®šè¯æ±‡å’Œè¯­æ³•ã€‚
- **é—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰ï¼š**
  è¿™ä¸ªç½‘ç»œè¾“å…¥ä¸€ç¯‡æ–°é—»æ–‡ç« ï¼Œå¹¶å†³å®šè¯¥æ–‡ç« åº”è¯¥ç”±å“ªäº›ä¸“å®¶è¿›è¡Œå¤„ç†ã€‚ä¾‹å¦‚ï¼š
  - å¦‚æœè¾“å…¥æ˜¯ä¸€æ¡çŸ­æ–°é—»ï¼ˆå¦‚æ¨æ–‡ï¼‰ï¼Œé—¨æ§ç½‘ç»œå¯èƒ½ä¼šæ›´å€¾å‘äº**Expert 1**ã€‚
  - å¦‚æœè¾“å…¥æ˜¯é•¿ç¯‡åˆ†ææŠ¥é“ï¼Œé—¨æ§ç½‘ç»œå¯èƒ½ä¼šæ›´å€¾å‘äº**Expert 2**ã€‚
  - å¦‚æœè¾“å…¥æ˜¯ç§‘æŠ€æ–°é—»ï¼Œé—¨æ§ç½‘ç»œå¯èƒ½ä¼šæ›´å€¾å‘äº**Expert 3**ã€‚
- **æœ€ç»ˆè¾“å‡ºï¼š**
  æ¯ä¸ªä¸“å®¶ç»™å‡ºè‡ªå·±çš„åˆ†ç±»é¢„æµ‹ï¼ŒMoEæ¨¡å‹ç»“åˆå„ä¸ªä¸“å®¶çš„è¾“å‡ºï¼Œå¹¶ç»™å‡ºæœ€ç»ˆçš„æ–‡æœ¬åˆ†ç±»ç»“æœã€‚

#### **ä¾‹å­2ï¼šå¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ MoE**

å‡è®¾æˆ‘ä»¬è¦è®­ç»ƒä¸€ä¸ªAIåŠ©æ‰‹ï¼Œå®ƒå¯ä»¥åŒæ—¶å¤„ç†**å›¾åƒã€æ–‡æœ¬ã€è¯­éŸ³**ç­‰å¤šç§è¾“å…¥æ•°æ®ã€‚

- **ä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰ï¼š**
  - **Expert 1**ï¼šä¸“æ³¨äºå¤„ç†æ–‡æœ¬ï¼ˆåŸºäºTransformerï¼‰ã€‚
  - **Expert 2**ï¼šä¸“æ³¨äºå¤„ç†å›¾åƒï¼ˆåŸºäºCNNæˆ–ViTï¼‰ã€‚
  - **Expert 3**ï¼šä¸“æ³¨äºå¤„ç†è¯­éŸ³ï¼ˆåŸºäºLSTMæˆ–Conformerï¼‰ã€‚
- **é—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰ï¼š**
  - å½“ç”¨æˆ·è¾“å…¥çš„æ˜¯æ–‡æœ¬æ—¶ï¼Œé—¨æ§ç½‘ç»œå¯èƒ½ä¼šæ›´å¤šæ¿€æ´»**Expert 1**ã€‚
  - å½“è¾“å…¥æ˜¯å›¾åƒæ—¶ï¼Œå®ƒå¯èƒ½ä¼šé€‰æ‹©**Expert 2**ã€‚
  - å½“è¾“å…¥æ˜¯è¯­éŸ³æ—¶ï¼Œå®ƒå¯èƒ½ä¼šé€‰æ‹©**Expert 3**ã€‚
  - å¯¹äºå¤šæ¨¡æ€è¾“å…¥ï¼ˆæ¯”å¦‚ä¸€ä¸ªå¸¦å›¾åƒçš„æ–‡æœ¬é—®é¢˜ï¼‰ï¼Œå®ƒå¯èƒ½ä¼šåŒæ—¶æ¿€æ´»å¤šä¸ªä¸“å®¶ï¼Œå¹¶èåˆä»–ä»¬çš„ç»“æœã€‚
- **æœ€ç»ˆè¾“å‡ºï¼š**
  ç»“åˆä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼ŒMoEç½‘ç»œå¯ä»¥ç»™å‡ºæ›´åŠ å‡†ç¡®çš„å›ç­”æˆ–æ¨èã€‚

#### **ä¾‹å­3ï¼šGoogle Switch Transformerï¼ˆè¶…å¤§è§„æ¨¡ MoEï¼‰**

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼ŒGoogle æå‡ºçš„ **Switch Transformer** æ˜¯ä¸€ç§å¤§è§„æ¨¡ MoE æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨è®­ç»ƒè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€BERTï¼‰æ—¶æé«˜è®¡ç®—æ•ˆç‡ã€‚

- è¿™ä¸ªæ¨¡å‹åŒ…å«**å¤šä¸ªä¸“å®¶Transformerå—**ï¼ˆä¾‹å¦‚ 32 ä¸ªï¼‰ã€‚
- ä½†æ˜¯**æ¯ä¸ªè¾“å…¥ token åªä¼šæ¿€æ´»å°‘æ•°å‡ ä¸ªä¸“å®¶ï¼ˆæ¯”å¦‚ 2 ä¸ªï¼‰**ï¼Œè€Œä¸æ˜¯è®©æ‰€æœ‰ 32 ä¸ªä¸“å®¶éƒ½è®¡ç®—ç»“æœã€‚
- è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š
  - **å‡å°‘è®¡ç®—æˆæœ¬**ï¼šä¸æ˜¯æ‰€æœ‰ä¸“å®¶éƒ½è¢«æ¿€æ´»ï¼Œè€Œæ˜¯ä»…è®¡ç®—æœ€ç›¸å…³çš„ä¸“å®¶ï¼ŒèŠ‚çœäº†ç®—åŠ›ã€‚
  - **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šä¸åŒä¸“å®¶æ“…é•¿ä¸åŒçš„ä»»åŠ¡ï¼Œæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„çŸ¥è¯†ã€‚

#### **æ€»ç»“**

MoE é€šè¿‡å¤šä¸ªä¸“å®¶æ¨¡å‹çš„ååŒå·¥ä½œï¼Œä½¿å¾—è®¡ç®—æ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶èƒ½é€‚åº”ä¸åŒçš„æ•°æ®åˆ†å¸ƒã€‚åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è¯­éŸ³è¯†åˆ«ã€å¤šæ¨¡æ€å­¦ä¹ ç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚

**Decoder-only æ¨¡å‹**æ˜¯ä¸€ç±» **è‡ªå›å½’ï¼ˆautoregressiveï¼‰** è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äº **æ–‡æœ¬ç”Ÿæˆä»»åŠ¡**ï¼Œæ¯”å¦‚ GPT ç³»åˆ—ã€LLaMAã€Mistral ç­‰ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ **é€æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ª token**ï¼ŒåŸºäºå·²ç»ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ã€‚

## 1.4 **Decoder-only ç»“æ„ç‰¹ç‚¹**

1. **è¾“å…¥ä»…æœ‰ Decoder**ï¼š
   - åªä½¿ç”¨ Transformer çš„ **è§£ç å™¨ï¼ˆDecoderï¼‰**ï¼Œæ²¡æœ‰ç¼–ç å™¨ï¼ˆEncoderï¼‰ã€‚
   - è¾“å…¥é€šå¸¸æ˜¯ä¸€ä¸ª **prompt**ï¼ˆå‰ç¼€æ–‡æœ¬ï¼‰ï¼Œæ¨¡å‹åŸºäºæ­¤ç”Ÿæˆæ–°çš„ tokenã€‚
2. **è‡ªå›å½’ç”Ÿæˆ**ï¼š
   - ä¾èµ–äº **Masked Self-Attention**ï¼Œç¡®ä¿æ¯ä¸ª token **åªèƒ½çœ‹åˆ°è‡ªå·±ä¹‹å‰çš„ token**ï¼Œä¸èƒ½çª¥æ¢æœªæ¥ã€‚
   - é€æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œç›´åˆ°æ»¡è¶³ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚ EOS token æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼‰ã€‚
3. **é€‚ç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡**ï¼š
   - é€‚ç”¨äº **æ–‡ç« ç»­å†™ã€å¯¹è¯ç”Ÿæˆã€ä»£ç ç”Ÿæˆ** ç­‰åœºæ™¯ã€‚

------

### **ä¸¾ä¾‹**

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ GPT-4 è¿™æ ·ä¸€ä¸ª **Decoder-only** æ¨¡å‹ï¼Œå¹¶ç»™å®šä¸€ä¸ª promptï¼š

#### **è¾“å…¥ Prompt**

```plaintext
æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§
```

#### **æ¨¡å‹è‡ªå›å½’ç”Ÿæˆ**

```
æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§ äººå·¥æ™ºèƒ½ æŠ€æœ¯ ï¼Œ å®ƒ é€šè¿‡ è®­ç»ƒ æ¨¡å‹ ä» æ•°æ® ä¸­ å­¦ä¹  è§„å¾‹ ã€‚
```

**ç”Ÿæˆè¿‡ç¨‹**ï¼š

1. **ç¬¬ä¸€æ­¥**ï¼šæ¨¡å‹æ¥æ”¶ `æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§`ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª token `äººå·¥æ™ºèƒ½`ã€‚
2. **ç¬¬äºŒæ­¥**ï¼šæ¥æ”¶ `æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§ äººå·¥æ™ºèƒ½`ï¼Œé¢„æµ‹ `æŠ€æœ¯`ã€‚
3. **ç¬¬ä¸‰æ­¥**ï¼šæ¥æ”¶ `æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§ äººå·¥æ™ºèƒ½ æŠ€æœ¯`ï¼Œé¢„æµ‹ `ï¼Œ`ã€‚
4. **ä¾æ¬¡ç»§ç»­â€¦â€¦**

è¿™ç§æ–¹å¼ä¸ **Encoder-Decoderï¼ˆå¦‚ T5, BARTï¼‰** çš„æ–¹æ³•ä¸åŒï¼Œåè€…é€šå¸¸ç”¨äº **åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ï¼‰**ã€‚

------

### **ä¸ Encoder-Decoder çš„å¯¹æ¯”**

| ç‰¹æ€§       | Decoder-only           | Encoder-Decoder     |
| ---------- | ---------------------- | ------------------- |
| ç»“æ„       | ä»… Decoder             | Encoder + Decoder   |
| é€‚ç”¨ä»»åŠ¡   | æ–‡æœ¬ç”Ÿæˆï¼ˆç»­å†™ã€å¯¹è¯ï¼‰ | æœºå™¨ç¿»è¯‘ã€æ‘˜è¦ç”Ÿæˆ  |
| æ³¨æ„åŠ›æœºåˆ¶ | Masked Self-Attention  | ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› |
| ç”Ÿæˆæ–¹å¼   | è‡ªå›å½’ç”Ÿæˆ             | ç¼–ç åå†è§£ç         |

------

### **åº”ç”¨**

- **GPT ç³»åˆ—ï¼ˆGPT-3, GPT-4, ChatGPTï¼‰**ï¼šå¯¹è¯ã€æ–‡ç« ç»­å†™
- **LLaMA, Mistral, Falcon**ï¼šå¼€æºå¤§æ¨¡å‹
- **CodeGPT, StarCoder**ï¼šä»£ç ç”Ÿæˆ

Decoder-only æ¨¡å‹åœ¨ **å¤§è§„æ¨¡é¢„è®­ç»ƒ+å¾®è°ƒ** æ–¹å¼ä¸‹ï¼Œè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶é€‚ç”¨äº **é•¿æ–‡æœ¬ç†è§£å’Œç”Ÿæˆä»»åŠ¡**ã€‚



## 1.5 **Feed Forward Networkï¼ˆå‰é¦ˆç¥ç»ç½‘ç»œï¼ŒFFNï¼‰**

å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯æœ€åŸºç¡€çš„äººå·¥ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼Œå®ƒçš„ç‰¹ç‚¹æ˜¯ä¿¡æ¯ä»è¾“å…¥å±‚å¼€å§‹ï¼Œç»è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚çš„åŠ æƒè®¡ç®—ï¼Œæœ€ç»ˆåˆ°è¾¾è¾“å‡ºå±‚ï¼Œä¸­é—´**æ²¡æœ‰å¾ªç¯æˆ–åé¦ˆè¿æ¥**ã€‚

------

### **ä¾‹å­ 1ï¼šç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œ**

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªä»»åŠ¡ï¼šå¯¹ä¸€ä¸ªäººçš„å¥åº·çŠ¶å†µè¿›è¡Œé¢„æµ‹ï¼ˆè¾“å‡º0ä»£è¡¨â€œä¸å¥åº·â€ï¼Œ1ä»£è¡¨â€œå¥åº·â€ï¼‰ã€‚è¾“å…¥ç‰¹å¾åŒ…æ‹¬ï¼š

- x1x_1 = å¹´é¾„
- x2x_2 = ä½“é‡
- x3x_3 = è¡€å‹

æˆ‘ä»¬æ­å»ºä¸€ä¸ª**ä¸¤å±‚å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼š

1. **è¾“å…¥å±‚**ï¼š3ä¸ªç¥ç»å…ƒï¼Œå¯¹åº”3ä¸ªè¾“å…¥ç‰¹å¾ã€‚
2. **éšè—å±‚**ï¼š4ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€‚
3. **è¾“å‡ºå±‚**ï¼š1ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°è¾“å‡ºå¥åº·æ¦‚ç‡ã€‚

#### **æ•°å­¦è¡¨ç¤º**

- **éšè—å±‚è®¡ç®—**ï¼š

  h=ReLU(W1â‹…x+b1)h = \text{ReLU}(W_1 \cdot x + b_1)

  å…¶ä¸­ï¼š

  - x=[x1,x2,x3]âŠ¤x = [x_1, x_2, x_3]^\top æ˜¯è¾“å…¥å‘é‡ã€‚
  - W1W_1 æ˜¯ä¸€ä¸ª 4Ã—34 \times 3 çš„æƒé‡çŸ©é˜µã€‚
  - b1b_1 æ˜¯ä¸€ä¸ª 4Ã—14 \times 1 çš„åç½®å‘é‡ã€‚

- **è¾“å‡ºå±‚è®¡ç®—**ï¼š

  y=Ïƒ(W2â‹…h+b2)y = \sigma(W_2 \cdot h + b_2)

  å…¶ä¸­ï¼š

  - W2W_2 æ˜¯ä¸€ä¸ª 1Ã—41 \times 4 çš„æƒé‡çŸ©é˜µã€‚
  - b2b_2 æ˜¯ä¸€ä¸ªåç½®é¡¹ï¼ˆæ ‡é‡ï¼‰ã€‚
  - Ïƒ\sigma æ˜¯Sigmoidå‡½æ•°ï¼Œå°†è¾“å‡ºå€¼é™åˆ¶åœ¨0åˆ°1ä¹‹é—´ã€‚

æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–æ–¹æ³•è®­ç»ƒè¿™ä¸ªç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹å¥åº·çŠ¶å†µã€‚

------

### **ä¾‹å­ 2ï¼šTransformer ä¸­çš„ FFN**

åœ¨**Transformer** ç»“æ„ï¼ˆå¦‚ BERTã€GPTï¼‰ä¸­ï¼Œæ¯ä¸ª**è‡ªæ³¨æ„åŠ›å±‚å**éƒ½ä¼šæœ‰ä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œç”¨æ¥è¿›è¡Œç‰¹å¾å˜æ¢ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯å‘é‡ xxï¼ˆç»´åº¦ä¸º ddï¼‰ï¼ŒTransformer ä¸­çš„ FFN è®¡ç®—å¦‚ä¸‹ï¼š

h=ReLU(W1â‹…x+b1)h = \text{ReLU}(W_1 \cdot x + b_1)y=W2â‹…h+b2y = W_2 \cdot h + b_2

å…¶ä¸­ï¼š

- W1W_1 æ˜¯ dhiddenÃ—dd_{\text{hidden}} \times d çš„çŸ©é˜µï¼ˆé€šå¸¸ dhiddend_{\text{hidden}} è¿œå¤§äº ddï¼Œå¦‚4å€ï¼‰ã€‚
- W2W_2 æ˜¯ dÃ—dhiddend \times d_{\text{hidden}} çš„çŸ©é˜µã€‚
- è¿™ä¸ªFFNä¸»è¦ä½œç”¨æ˜¯**å¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›**ï¼Œè®©æ¨¡å‹èƒ½æ›´å¥½åœ°å­¦ä¹ å¤æ‚çš„æ˜ å°„å…³ç³»ã€‚

------

### **æ€»ç»“**

1. **å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFFNï¼‰** æ˜¯æœ€åŸºæœ¬çš„ç¥ç»ç½‘ç»œï¼Œä¿¡æ¯å•å‘æµåŠ¨ï¼Œæ²¡æœ‰å¾ªç¯æˆ–åé¦ˆã€‚

2. **è®¡ç®—æ­¥éª¤**ï¼šè¾“å…¥å±‚ â†’ åŠ æƒæ±‚å’Œ â†’ æ¿€æ´»å‡½æ•° â†’ è¾“å‡ºå±‚ã€‚

3. åº”ç”¨

   ï¼š

   - ç»å…¸ç¥ç»ç½‘ç»œï¼Œå¦‚MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰ã€‚
   - ç°ä»£æ·±åº¦å­¦ä¹ ï¼Œå¦‚Transformerä¸­çš„FFNéƒ¨åˆ†ã€‚

å¸Œæœ›è¿™ä¸ªè§£é‡Šèƒ½å¸®åŠ©ä½ ç†è§£FFNï¼å¦‚æœæœ‰æ›´å…·ä½“çš„åº”ç”¨é—®é¢˜ï¼Œæ¬¢è¿è®¨è®ºï¼





## 1.6 Gradient Checkpointing ä»‹ç»ä¸ç¤ºä¾‹

#### 1. **Gradient Checkpointing æ˜¯ä»€ä¹ˆï¼Ÿ**

Gradient Checkpointingï¼ˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰æ˜¯ä¸€ç§ **èŠ‚çœå†…å­˜çš„è®­ç»ƒæŠ€å·§**ï¼Œç”¨äº **æ·±åº¦ç¥ç»ç½‘ç»œ**ã€‚å®ƒé€šè¿‡ **å­˜å‚¨éƒ¨åˆ†ä¸­é—´æ¿€æ´»å€¼**ï¼Œåœ¨åå‘ä¼ æ’­æ—¶ **é‡æ–°è®¡ç®—æœªå­˜å‚¨çš„æ¿€æ´»å€¼**ï¼Œä»è€Œå‡å°‘ GPU/å†…å­˜çš„å ç”¨ï¼Œé€‚ç”¨äº **æ·±åº¦æ¨¡å‹ï¼ˆå¦‚ Transformer, GPT, BERTï¼‰** è®­ç»ƒã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š

- åœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œä¸å­˜å‚¨æ‰€æœ‰å±‚çš„æ¿€æ´»å€¼ï¼Œè€Œæ˜¯åªå­˜å‚¨ **éƒ¨åˆ†å…³é”®å±‚ï¼ˆCheckpointsï¼‰** çš„æ¿€æ´»å€¼ã€‚
- åœ¨åå‘ä¼ æ’­æ—¶ï¼Œå¯¹æœªå­˜å‚¨çš„æ¿€æ´»å€¼ **é‡æ–°è®¡ç®—**ï¼ŒèŠ‚çœæ˜¾å­˜ä½†å¢åŠ äº†ä¸€äº›è®¡ç®—é‡ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š

- **è¶…æ·±ç¥ç»ç½‘ç»œ**ï¼ˆå¦‚ 100 å±‚ä»¥ä¸Šçš„ Transformerã€ResNetï¼‰ã€‚
- **å†…å­˜å—é™çš„ GPU è®­ç»ƒ**ï¼ˆå¦‚è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼‰ã€‚
- **å¾®è°ƒï¼ˆFine-tuningï¼‰å¤§æ¨¡å‹**ï¼ˆå¦‚ LoRA è®­ç»ƒ BERT, GPT ç­‰ï¼‰ã€‚

------

#### 2. **PyTorch å®ç° Gradient Checkpointing**

PyTorch æä¾›äº† `torch.utils.checkpoint.checkpoint` æ–¹ä¾¿å®ç°æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ª **ç®€å•çš„ç¤ºä¾‹**ï¼Œå±•ç¤ºå¦‚ä½•åœ¨ ResNet-like ç½‘ç»œä¸­ä½¿ç”¨ Gradient Checkpointingï¼š

##### **(1) ä¸ä½¿ç”¨ Gradient Checkpointing**

```python
import torch
import torch.nn as nn
import torch.optim as optim

class DeepModel(nn.Module):
    def __init__(self):
        super(DeepModel, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(1000, 1000),
            nn.ReLU(),
            nn.Linear(1000, 1000),
            nn.ReLU(),
            nn.Linear(1000, 10)
        )

    def forward(self, x):
        return self.layers(x)

# åˆ›å»ºæ¨¡å‹ & è®­ç»ƒ
model = DeepModel().cuda()
optimizer = optim.Adam(model.parameters(), lr=0.001)

x = torch.randn(1, 1000).cuda()
output = model(x).sum()
output.backward()
```

> **é—®é¢˜**ï¼šæ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼éƒ½å­˜å‚¨åœ¨ GPU ä¸Šï¼Œæ¶ˆè€—å¤§é‡æ˜¾å­˜ã€‚

------

##### **(2) ä½¿ç”¨ Gradient Checkpointing**

```python
import torch.utils.checkpoint as checkpoint

class DeepModelCheckpoint(nn.Module):
    def __init__(self):
        super(DeepModelCheckpoint, self).__init__()
        self.layer1 = nn.Linear(1000, 1000)
        self.layer2 = nn.Linear(1000, 1000)
        self.layer3 = nn.Linear(1000, 10)

    def forward(self, x):
        x = checkpoint.checkpoint(self.layer1, x)  # åªå­˜å‚¨å…³é”®å±‚çš„æ¿€æ´»
        x = torch.relu(x)
        x = checkpoint.checkpoint(self.layer2, x)
        x = torch.relu(x)
        x = self.layer3(x)
        return x

# è®­ç»ƒ
model = DeepModelCheckpoint().cuda()
output = model(x).sum()
output.backward()
```

> **ä¼˜ç‚¹**ï¼šå‡å°‘ GPU å†…å­˜å ç”¨ï¼Œä½†å¢åŠ äº†ä¸€äº›è®¡ç®—æ—¶é—´ï¼ˆå› ä¸ºæœªå­˜å‚¨çš„æ¿€æ´»å€¼éœ€è¦åœ¨åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ï¼‰ã€‚

------

#### 3. **Gradient Checkpointing åœ¨ Transformer ä¸­çš„åº”ç”¨**

åœ¨ Transformerï¼ˆå¦‚ BERT, GPTï¼‰ç­‰å¤§æ¨¡å‹ä¸­ï¼ŒGradient Checkpointing ä¹Ÿè¢«å¹¿æ³›ä½¿ç”¨ï¼š

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-uncased")
model.gradient_checkpointing_enable()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
```

> **æ•ˆæœ**ï¼šèƒ½å¤§å¹…é™ä½ BERT/GPT ç­‰æ¨¡å‹çš„æ˜¾å­˜å ç”¨ï¼Œä½¿å¤§æ¨¡å‹åœ¨ **æ›´å°çš„æ˜¾å­˜** ä¸Šè®­ç»ƒã€‚

------

#### 4. **æ€»ç»“**

âœ… **ä¼˜ç‚¹**ï¼š

- **èŠ‚çœæ˜¾å­˜**ï¼Œé€‚ç”¨äº **å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ æ¨¡å‹**ã€‚
- **æ”¯æŒä»»æ„è®¡ç®—å›¾**ï¼Œä¸å½±å“å‰å‘ä¼ æ’­é€»è¾‘ã€‚

âš ï¸ **ç¼ºç‚¹**ï¼š

- **å¢åŠ è®¡ç®—é‡**ï¼ˆå› éƒ¨åˆ†å‰å‘ä¼ æ’­éœ€è¦ **é‡æ–°è®¡ç®—**ï¼‰ã€‚
- **ä¸é€‚ç”¨äºæ‰€æœ‰å±‚**ï¼ˆå¦‚ BatchNorm ä¸èƒ½ç”¨ï¼Œå› ä¸ºå®ƒéœ€è¦ä¿ç•™çŠ¶æ€ï¼‰ã€‚

ğŸ’¡ **ä½•æ—¶ä½¿ç”¨ï¼Ÿ**

- è®­ç»ƒ **è¶…æ·±ç¥ç»ç½‘ç»œ**ï¼ˆå¦‚ GPT-4, BERTï¼‰ã€‚
- æ˜¾å­˜ä¸è¶³æ—¶ï¼ˆå°¤å…¶æ˜¯ **å¾®è°ƒ LLM**ï¼‰ã€‚

------

è¿™æ ·ï¼Œä½ å¯ä»¥åœ¨æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ **å¹³è¡¡æ˜¾å­˜ä¸è®¡ç®—æ•ˆç‡**ï¼Œè®©æ¨¡å‹åœ¨å—é™ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œï¼ğŸš€



# 2.Qwenä»£ç å®ç°

## 2.1ç¯å¢ƒ

å¯¼å…¥äº†ä¸‰ä¸ª Python æ¨¡å—ï¼Œå®ƒä»¬éƒ½æ˜¯ **PyTorch** ç”Ÿæ€ä¸­çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚è®©æˆ‘é€ä¸ªè§£é‡Šå®ƒä»¬çš„ç”¨é€”ï¼š

------

### **1. `import math`**

- `math` æ˜¯ Python çš„æ ‡å‡†æ•°å­¦åº“ï¼Œæä¾›äº†ä¸€äº›**æ•°å­¦å‡½æ•°**å’Œå¸¸é‡ï¼ˆå¦‚ `math.pi`, `math.exp()`, `math.log()` ç­‰ï¼‰ã€‚

- åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå®ƒé€šå¸¸ç”¨äº

  è®¡ç®—è§’åº¦ã€æŒ‡æ•°ã€å¯¹æ•°ç­‰æ•°å­¦è¿ç®—

  ï¼Œä¾‹å¦‚ï¼š

  ```python
  import math
  angle = math.pi / 4  # è®¡ç®— Ï€/4
  result = math.sin(angle)  # è®¡ç®—æ­£å¼¦å€¼
  ```

------

### **2. `import torch`**

- `torch` æ˜¯ **PyTorch** çš„ä¸»æ¨¡å—ï¼Œæä¾› **å¼ é‡ï¼ˆTensorï¼‰è®¡ç®—** å’Œ **è‡ªåŠ¨æ±‚å¯¼** æœºåˆ¶ã€‚

- ä½ å¯ä»¥åˆ›å»ºã€æ“ä½œå¼ é‡ï¼Œå¹¶åœ¨ GPU/CPU ä¹‹é—´åˆ‡æ¢ï¼š

  ```python
  import torch
  x = torch.tensor([1.0, 2.0, 3.0])  # åˆ›å»ºä¸€ä¸ªå¼ é‡
  y = x * 2  # å¼ é‡è¿ç®—
  ```

------

### **3. `import torch.nn as nn`**

- `torch.nn` æ˜¯ **PyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å—**ï¼Œæä¾›å¸¸ç”¨çš„ **ç¥ç»ç½‘ç»œå±‚ï¼ˆLayerï¼‰** å’Œ **æ¿€æ´»å‡½æ•°**ã€‚

- `nn` æ˜¯å®ƒçš„åˆ«åï¼Œæ–¹ä¾¿è°ƒç”¨ã€‚

- å¸¸è§çš„å±‚åŒ…æ‹¬ï¼š

  ```python
  import torch.nn as nn
  
  linear_layer = nn.Linear(10, 5)  # åˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚ï¼ˆè¾“å…¥ 10 ç»´ï¼Œè¾“å‡º 5 ç»´ï¼‰
  relu = nn.ReLU()  # ReLU æ¿€æ´»å‡½æ•°
  ```

------

### **4. `import torch.nn.functional as F`**

- `torch.nn.functional` æä¾›äº†ä¸€äº›**ä¸å¸¦å‚æ•°çš„å‡½æ•°**ï¼ˆå¦‚æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ï¼‰ã€‚

- å’Œ 

  ```
  torch.nn
  ```

   çš„åŒºåˆ«æ˜¯ï¼š

  - `torch.nn` æä¾›çš„æ˜¯**å¯è®­ç»ƒå±‚**ï¼ˆå¸¦ `weight` å’Œ `bias`ï¼‰ã€‚
  - `torch.nn.functional` æä¾›çš„æ˜¯**å‡½æ•°å¼ API**ï¼Œä¸ä¼šè‡ªåŠ¨ç®¡ç†æƒé‡ã€‚

- ä¾‹å¦‚ï¼š

  ```python
  import torch.nn.functional as F
  
  x = torch.tensor([-1.0, 0.0, 1.0])
  y = F.relu(x)  # ç›´æ¥è°ƒç”¨ ReLU
  print(y)  # tensor([0., 0., 1.])
  ```

- `F.relu(x)` **ä¸ä¼šåˆ›å»ºé¢å¤–çš„æƒé‡**ï¼Œä½† `nn.ReLU()` ä¼šã€‚

------

### **æ€»ç»“**

| å¯¼å…¥æ¨¡å—              | ä½œç”¨                                           |
| --------------------- | ---------------------------------------------- |
| `math`                | Python æ ‡å‡†æ•°å­¦åº“ï¼ˆæŒ‡æ•°ã€å¯¹æ•°ã€ä¸‰è§’å‡½æ•°ç­‰ï¼‰    |
| `torch`               | PyTorch æ ¸å¿ƒï¼Œæ”¯æŒå¼ é‡è®¡ç®—å’Œ GPU åŠ é€Ÿ          |
| `torch.nn`            | PyTorch **ç¥ç»ç½‘ç»œå±‚**ï¼ˆçº¿æ€§å±‚ã€å·ç§¯å±‚ç­‰ï¼‰     |
| `torch.nn.functional` | PyTorch **å‡½æ•°å¼ API**ï¼ˆæ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ç­‰ï¼‰ |

ğŸ’¡ **é€‚ç”¨åœºæ™¯**

- **`torch.nn`** é€‚ç”¨äº**æ„é€ ç¥ç»ç½‘ç»œ**ï¼ˆè‡ªåŠ¨ç®¡ç†æƒé‡ï¼‰ã€‚
- **`torch.nn.functional`** é€‚ç”¨äº**ä¸´æ—¶è®¡ç®—**ï¼ˆä¸ä¿å­˜å‚æ•°ï¼‰ã€‚
- **`math`** é€‚ç”¨äºä¸€èˆ¬æ•°å­¦è®¡ç®—ï¼ˆä¸æ¶‰åŠ PyTorch è®¡ç®—å›¾ï¼‰ã€‚





## 2.2 é…ç½®ç±»

**`Qwen2Config`** é…ç½®ç±»ï¼Œé€šå¸¸ç”¨äºå­˜å‚¨å¹¶ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆç‰¹åˆ«æ˜¯ Transformer æ¨¡å‹ï¼‰çš„è¶…å‚æ•°ã€‚

------

### **ä»£ç è§£æ**

#### **1. ä¸»è¦ç”¨é€”**

- è¯¥ç±»çš„ä½œç”¨æ˜¯**å­˜å‚¨å¹¶ä¼ é€’è¶…å‚æ•°**ï¼Œç”¨äºæ„å»º Transformer ç»“æ„çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚ GPTã€BERTï¼‰ã€‚
- é€šè¿‡åˆå§‹åŒ–å‚æ•°ï¼Œå¯ä»¥æ§åˆ¶ Transformer ç»“æ„çš„å±‚æ•°ã€éšè—ç»´åº¦ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰é…ç½®ã€‚

#### **2. `__init__` æ–¹æ³•**

æ„é€ å‡½æ•° `__init__()` è´Ÿè´£åˆå§‹åŒ–æ¨¡å‹é…ç½®ï¼Œæ‰€æœ‰å‚æ•°éƒ½å¯ä»¥ä¿®æ”¹ï¼Œå…è®¸ç”¨æˆ·**è‡ªå®šä¹‰è¶…å‚æ•°**ã€‚

| å‚æ•°                           | è¯´æ˜                                                         |
| ------------------------------ | ------------------------------------------------------------ |
| `vocab_size=30522`             | è¯æ±‡è¡¨å¤§å°ï¼Œé»˜è®¤å€¼ `30522`ï¼ˆå¸¸ç”¨äº BERTã€GPT å˜ä½“ï¼‰ã€‚        |
| `hidden_size=768`              | Transformer éšè—å±‚ï¼ˆembeddingï¼‰ç»´åº¦ï¼Œå†³å®šæ¯ä¸ª token çš„è¡¨ç¤ºå¤§å°ã€‚ |
| `num_hidden_layers=12`         | Transformer å±‚æ•°ï¼Œå†³å®šäº†æ·±åº¦ï¼Œé€šå¸¸è¶Šå¤§æ¨¡å‹èƒ½åŠ›è¶Šå¼ºä½†è®¡ç®—é‡è¶Šå¤§ã€‚ |
| `num_attention_heads=12`       | å¤šå¤´è‡ªæ³¨æ„åŠ›çš„å¤´æ•°ã€‚                                         |
| `num_key_value_heads=4`        | **GQAï¼ˆGrouped Query Attentionï¼‰ç›¸å…³**ï¼Œç”¨äºå‡å°‘è®¡ç®—é‡ã€‚     |
| `intermediate_size=3072`       | MLP å±‚çš„ä¸­é—´ç»´åº¦ï¼Œé€šå¸¸æ˜¯ `hidden_size` çš„ 4 å€ï¼ˆå¦‚ `3072 = 4 * 768`ï¼‰ã€‚ |
| `max_position_embeddings=2048` | æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ `2048`ï¼ˆGPT-3 åŠå¤§éƒ¨åˆ† LLM ä½¿ç”¨è¯¥å€¼ï¼‰ã€‚ |
| `rope_theta=10000.0`           | **RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰**çš„ `theta` å‚æ•°ï¼Œæ§åˆ¶ä½ç½®ç¼–ç çš„é¢‘ç‡å‚æ•°ã€‚ |
| `attention_dropout=0.1`        | è‡ªæ³¨æ„åŠ›å±‚çš„ Dropout ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚                        |
| `hidden_act="silu"`            | MLP æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º `SiLU`ï¼ˆSwish å˜ä½“ï¼‰ã€‚                  |
| `attention_bias=False`         | æ§åˆ¶ QKV æŠ•å½±æ˜¯å¦ä½¿ç”¨ `bias`ï¼ˆå¯ä»¥å‡å°‘å‚æ•°é‡ï¼‰ã€‚             |
| `rms_norm_eps=1e-6`            | **RMSNorm** å±‚çš„ epsilon å€¼ï¼Œé˜²æ­¢æ•°å€¼è®¡ç®—é—®é¢˜ï¼ˆå¦‚é™¤é›¶é”™è¯¯ï¼‰ã€‚ |
| `pad_token_id=0`               | `pad` token IDï¼Œä¸€èˆ¬ç”¨äºå¡«å……ï¼ˆå¦‚ NLP ä»»åŠ¡ä¸­çš„ `PAD`ï¼‰ã€‚      |
| `_attn_implementation="eager"` | **æ³¨æ„åŠ›å®ç°æ–¹å¼**ï¼Œå¯ä»¥é€‰æ‹© `"eager"`ï¼ˆä¼ ç»Ÿè®¡ç®—ï¼‰ã€`"flash_attention_2"`ï¼ˆæ›´é«˜æ•ˆï¼‰ã€`"sdpa"`ï¼ˆscaled dot-product attentionï¼‰ã€‚ |

#### **3. é¢å¤–å±æ€§**

```python
self.gradient_checkpointing = False
```

- **`gradient_checkpointing`**ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè‹¥ `True`ï¼Œå¯å‡å°‘æ˜¾å­˜æ¶ˆè€—ï¼ˆé€‚ç”¨äºè®­ç»ƒå¤§æ¨¡å‹ï¼‰ã€‚

------

### **æ ¸å¿ƒæ¦‚å¿µè§£æ**

#### **1. Transformer ç›¸å…³å‚æ•°**

- `hidden_size`ï¼šTransformer çš„**éšè—ç»´åº¦**ï¼Œå†³å®šè¯å‘é‡çš„å¤§å°ã€‚
- `num_hidden_layers`ï¼šTransformer å±‚æ•°ï¼ˆ**æ·±åº¦**ï¼‰ã€‚
- `num_attention_heads`ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›çš„**å¤´æ•°**ï¼Œå½±å“æ³¨æ„åŠ›è®¡ç®—çš„å¹¶è¡Œåº¦ã€‚
- `num_key_value_heads`ï¼š**GQAï¼ˆGrouped Query Attentionï¼‰**ç›¸å…³å‚æ•°ï¼Œå‡å°‘è®¡ç®—é‡ã€‚

#### **2. RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰**

- **`rope_theta`** æ˜¯ `RoPE`ï¼ˆRotary Position Embeddingï¼‰çš„è¶…å‚æ•°ï¼Œå†³å®šä½ç½®ç¼–ç çš„å‘¨æœŸæ€§ã€‚
- `RoPE` ä¸»è¦ç”¨äºæ›¿ä»£ `absolute positional encoding`ï¼Œèƒ½å¤Ÿæå‡é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚

#### **3. `hidden_act="silu"`**

- `SiLU`ï¼ˆ`Swish` çš„å˜ä½“ï¼‰æ˜¯ä¸€ç§å¹³æ»‘çš„æ¿€æ´»å‡½æ•°ï¼Œç›¸è¾ƒäº ReLU å¯æå‡æ¨¡å‹æ•ˆæœï¼š SiLU(x)=xâ‹…Ïƒ(x)
- å…¶ä¸­ Ïƒ(x)æ˜¯ sigmoid å‡½æ•°ã€‚

#### **4. `attention_bias`**

- åœ¨ QKV è®¡ç®—æ—¶ï¼Œæ˜¯å¦åŠ ä¸Š `bias`ï¼Œè®¾ä¸º `False` å¯å‡å°‘å‚æ•°é‡ï¼ŒèŠ‚çœæ˜¾å­˜ã€‚

#### **5. `_attn_implementation`**

- `eager`ï¼šæ™®é€š PyTorch å®ç°ï¼ˆæ…¢ï¼Œä½†å…¼å®¹æ€§å¼ºï¼‰ã€‚
- `flash_attention_2`ï¼šFlash Attention 2ï¼ŒåŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚
- `sdpa`ï¼š`scaled dot-product attention`ï¼ŒPyTorch çš„é«˜æ•ˆæ³¨æ„åŠ›å®ç°ã€‚

------

### **æ€»ç»“**

- è¯¥ `Qwen2Config` é…ç½®ç±»ä¸»è¦æ˜¯**ä¸º Transformer æä¾›è¶…å‚æ•°**ï¼Œç”¨äºåˆå§‹åŒ–æ¨¡å‹ã€‚
- è¯¥ç±»é€‚ç”¨äº GPT å˜ä½“ï¼Œæ”¯æŒ `RoPE` ä½ç½®ç¼–ç ã€GQAï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰ï¼Œå¯é€‰ `flash_attention_2` åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ã€‚
- è¯¥é…ç½®ç±»æä¾›**çµæ´»æ€§**ï¼Œç”¨æˆ·å¯è‡ªå®šä¹‰ `hidden_size`ã€`num_hidden_layers`ã€`attention_heads` ç­‰å‚æ•°æ¥é€‚é…ä¸åŒè®¡ç®—èµ„æºã€‚



## 2.3 é¢„è®­ç»ƒæ¨¡å‹åŸºç±» (ç®€åŒ–ç‰ˆ)

è¯¥ç±» **`Qwen2PreTrainedModel`** ç»§æ‰¿è‡ª `torch.nn.Module`ï¼Œæ˜¯ **Qwen2 æ¨¡å‹çš„é¢„è®­ç»ƒåŸºç±»**ã€‚

å®ƒæä¾›äº†ä¸€äº›åŸºç¡€åŠŸèƒ½ï¼Œå¦‚ï¼š

1. **å­˜å‚¨é…ç½® (`config`)**
2. **åˆå§‹åŒ–æƒé‡ (`init_weights`)**
3. **æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ (`gradient_checkpointing`)**
4. **åå¤„ç† (`post_init`)**

### **1. `__init__` æ„é€ å‡½æ•°**

```python
def __init__(self, config: Qwen2Config):
    super().__init__()
    self.config = config
```

- è¯¥æ„é€ å‡½æ•°æ¥æ”¶ä¸€ä¸ª **`Qwen2Config` é…ç½®å¯¹è±¡**ï¼Œç”¨äºå­˜å‚¨**æ¨¡å‹çš„è¶…å‚æ•°**ã€‚
- é€šè¿‡ `super().__init__()` è°ƒç”¨ `torch.nn.Module` çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç¡®ä¿ `self.named_parameters()` ç­‰æ–¹æ³•å¯ç”¨ã€‚

------

### **2. `init_weights()`**

```python
def init_weights(self):
    """
    ç®€åŒ–çš„æƒé‡åˆå§‹åŒ–é€»è¾‘ï¼Œä¹Ÿå¯ä½¿ç”¨ xavier_uniformã€kaiming_uniform ç­‰æ›´å¤æ‚åˆå§‹åŒ–ã€‚
    """
    for name, param in self.named_parameters():
        if param.dim() > 1:
            nn.init.xavier_uniform_(param)
```

#### **ä½œç”¨**

- **åˆå§‹åŒ–æ¨¡å‹æƒé‡**ï¼Œç¡®ä¿æ¨¡å‹è®­ç»ƒæ—¶çš„æƒé‡å¤„äºåˆé€‚çš„èŒƒå›´ã€‚

- è¿™é‡Œä½¿ç”¨çš„æ˜¯ 

  `xavier_uniform_`

   æ–¹æ³•ï¼š

  - é€‚ç”¨äº **`tanh` å’Œ `sigmoid` æ¿€æ´»å‡½æ•°**ã€‚
  - è®¡ç®—å…¬å¼ï¼š![image-20250210230826065](D:\Document\GitHub\White-box\README.assets\image-20250210230826065.png)
  - å…¶ä¸­ ninå’Œ noutåˆ†åˆ«æ˜¯è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°ã€‚

#### **æ”¹è¿›å»ºè®®**

å¯ä»¥æ”¹æˆï¼š

```python
nn.init.kaiming_uniform_(param, nonlinearity="relu")
```

- é€‚ç”¨äº `ReLU` æ¿€æ´»å‡½æ•°
- é€‚åˆ **æ·±åº¦ç½‘ç»œï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±**

------

### **3. `_backward_compatibility_gradient_checkpointing()`**

```python
def _backward_compatibility_gradient_checkpointing(self):
    """
    å…¼å®¹ä¸€äº›è€ç‰ˆæœ¬æˆ–è€… transformers å†…éƒ¨çš„æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾å®šã€‚
    """
    self.gradient_checkpointing = self.config.gradient_checkpointing
```

#### **ä½œç”¨**

- æ¢¯åº¦æ£€æŸ¥ç‚¹

  ï¼ˆGradient Checkpointingï¼‰æ˜¯ä¸€ç§ 

  å‡å°‘æ˜¾å­˜å ç”¨

   çš„æŠ€æœ¯ï¼š

  - åœ¨**å‰å‘ä¼ æ’­**æ—¶ä¸å­˜å‚¨æ‰€æœ‰ä¸­é—´ç»“æœï¼Œè€Œæ˜¯ **åªå­˜å‚¨å°‘éƒ¨åˆ†å…³é”®å˜é‡**ã€‚
  - åœ¨**åå‘ä¼ æ’­**æ—¶é‡æ–°è®¡ç®—ä¸¢å¼ƒçš„ä¸­é—´å€¼ï¼Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œä½†å¢åŠ è®¡ç®—é‡ã€‚

- è¿™é‡Œçš„å‡½æ•°ä¿è¯ `gradient_checkpointing` **ä¸ config è®¾ç½®ä¿æŒä¸€è‡´**ï¼Œé¿å…è€ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ã€‚

------

### **4. `post_init()`**

```python
def post_init(self):
    """
    åˆå§‹åŒ–ç»“æŸåçš„å‡½æ•°ï¼Œä¸€èˆ¬ç”¨äºæƒé‡åˆå§‹åŒ–å’Œå…¶ä»–å…¼å®¹æ€§æ£€æŸ¥ã€‚
    """
    self.init_weights()
    self._backward_compatibility_gradient_checkpointing()
```

#### **ä½œç”¨**

- ```
  post_init()
  ```

   åœ¨æ¨¡å‹åˆå§‹åŒ–åè°ƒç”¨ï¼Œæ‰§è¡Œï¼š

  1. **æƒé‡åˆå§‹åŒ–**
  2. **æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾å®š**

- è¿™ç§ **åˆå§‹åŒ–åæ‰§è¡Œçš„æ¨¡å¼** å¸¸è§äº **Transformer æ¨¡å‹**ï¼ˆå¦‚ `Hugging Face` çš„ `transformers`ï¼‰ã€‚

------

###  **æ€»ç»“**

| æ–¹æ³•                                               | ä½œç”¨                                       |
| -------------------------------------------------- | ------------------------------------------ |
| `__init__()`                                       | ä¼ å…¥ `config` é…ç½®ï¼Œå¹¶åˆå§‹åŒ–æ¨¡å‹           |
| `init_weights()`                                   | åˆå§‹åŒ–æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨ `xavier_uniform_`     |
| `_backward_compatibility_gradient_checkpointing()` | å…¼å®¹æ—§ç‰ˆæœ¬çš„ `gradient_checkpointing` è®¾å®š |
| `post_init()`                                      | è¿›è¡Œæƒé‡åˆå§‹åŒ–å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹è®¾å®š             |

**ğŸ’¡ è¿™ä¸ªåŸºç±»æ˜¯ Transformer é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ï¼Œå…·ä½“æ¨¡å‹ï¼ˆå¦‚ GPT å˜ä½“ï¼‰ä¼šç»§æ‰¿å®ƒï¼Œå¹¶å®ç°å…·ä½“çš„å‰å‘ä¼ æ’­ (`forward`) é€»è¾‘ï¼** ğŸš€



## 2.4 Qwen2æ¨¡å‹ä¸»ä½“ Qwen2Model

è¯¥ç±» **`Qwen2Model`** ç»§æ‰¿è‡ª `Qwen2PreTrainedModel`ï¼Œ

æ˜¯ä¸€ä¸ªå®Œæ•´çš„ **Transformer Decoder-onlyï¼ˆè§£ç å™¨ï¼‰** æ¨¡å‹ï¼ˆç±»ä¼¼ GPTï¼‰ã€‚

å®ƒçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. **è¯åµŒå…¥ï¼ˆToken Embeddingï¼‰**
2. **å¤šå±‚ Transformer è§£ç å™¨ï¼ˆDecoder Layersï¼‰**
3. **å½’ä¸€åŒ–å±‚ï¼ˆNormalization Layerï¼‰**
4. **æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰**
5. **å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰**

è¿™ä¸ªæ¨¡å‹çš„è®¾è®¡æ€è·¯ä¸ GPT ç±»ä¼¼ï¼Œé€‚ç”¨äº **æ–‡æœ¬ç”Ÿæˆä»»åŠ¡**ï¼ˆå¦‚ **LLM è¯­è¨€æ¨¡å‹**ï¼‰ã€‚

### **1. `__init__()` æ„é€ å‡½æ•°**

```python
def __init__(self, config: Qwen2Config):
    super().__init__(config)
    self.padding_idx = config.pad_token_id
    self.vocab_size = config.vocab_size
```

#### **ä½œç”¨**

- ç»§æ‰¿ `Qwen2PreTrainedModel` å¹¶åˆå§‹åŒ–æ¨¡å‹
- è¯»å– `pad_token_id`ï¼ˆå¡«å…… token IDï¼‰ï¼Œç”¨äº `nn.Embedding` å±‚çš„ `padding_idx`
- è¯»å– `vocab_size`ï¼ˆè¯è¡¨å¤§å°ï¼‰

------

### **2. è¯åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰**

```python
# è¯å‘é‡Embedding
self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
```

#### **ä½œç”¨**

- `nn.Embedding` å°† **Token ID** æ˜ å°„ä¸º **å‘é‡**ï¼ˆ`hidden_size` ç»´åº¦ï¼‰
- `padding_idx=config.pad_token_id` è®© `PAD` token **ä¸ä¼šæ›´æ–°æ¢¯åº¦**

> **ç¤ºä¾‹**

```python
# åˆ›å»ºä¸€ä¸ª Embedding å±‚
embedding_layer = nn.Embedding(10000, 768, padding_idx=0)

# ç”Ÿæˆä¸€ä¸ª 2 å¥ 4 è¯çš„è¾“å…¥
input_ids = torch.tensor([[1, 2, 3, 4], [5, 6, 0, 0]])  # (2, 4)

# è¯åµŒå…¥
embeds = embedding_layer(input_ids)  # (2, 4, 768)
```

------

### **3. Transformer è§£ç å™¨å±‚**

```python
# Decoderå±‚ï¼Œå­˜ num_hidden_layers ä¸ª
self.layers = nn.ModuleList(
    [Qwen2DecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]
)
```

#### **ä½œç”¨**

- `nn.ModuleList` å­˜å‚¨ **å¤šä¸ª Transformer è§£ç å™¨å±‚**
- `num_hidden_layers` æ§åˆ¶ **æ¨¡å‹æ·±åº¦**
- `Qwen2DecoderLayer`ï¼ˆæœªæä¾›ä»£ç ï¼‰**æ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ª Transformer è§£ç å™¨å±‚**

> **ç¤ºä¾‹**

```python
# 12 å±‚è§£ç å™¨
model.layers[0]  # ç¬¬ä¸€å±‚ Transformer Decoder
```

------

### **4. å½’ä¸€åŒ–å±‚ï¼ˆRMSNormï¼‰**

```python
self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

#### **ä½œç”¨**

- `RMSNorm` æ˜¯ **æ¯” LayerNorm æ›´é€‚åˆ Transformer** çš„å½’ä¸€åŒ–å±‚
- RMSNorm è®¡ç®—å…¬å¼ï¼š y=x1dâˆ‘i=1dxi2+Ïµy = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} å…¶ä¸­ dd æ˜¯éšè—ç»´åº¦ã€‚

------

### **5. æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**

```python
self.gradient_checkpointing = config.gradient_checkpointing
```

- ```
  gradient_checkpointing=True
  ```

   æ—¶ï¼š

  - **å‰å‘ä¼ æ’­**æ—¶ä¸¢å¼ƒä¸­é—´å±‚ç¼“å­˜ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
  - **åå‘ä¼ æ’­**æ—¶é‡æ–°è®¡ç®—æ¢¯åº¦ï¼ˆå¢åŠ è®¡ç®—é‡ï¼‰

------

### **6. æƒé‡åˆå§‹åŒ–**

```python
self.post_init()
```

- ç»§æ‰¿çš„ 

  ```
  post_init()
  ```

   è´Ÿè´£ï¼š

  - **æƒé‡åˆå§‹åŒ–**
  - **æ¢¯åº¦æ£€æŸ¥ç‚¹å…¼å®¹æ€§è®¾å®š**

------

## **`forward()`ï¼šå‰å‘ä¼ æ’­**

```python
def forward(
    self,
    input_ids=None,
    attention_mask=None,
    position_ids=None,
    output_hidden_states=False,
    output_attentions=False,
    use_cache=False,
    past_key_value=None
):
```

### **è¾“å…¥å‚æ•°**

| å‚æ•°                   | è¯´æ˜                                                       |
| ---------------------- | ---------------------------------------------------------- |
| `input_ids`            | è¾“å…¥ Token åºåˆ— (`[batch_size, seq_len]`)                  |
| `attention_mask`       | æ©ç  (`[batch_size, 1, seq_len, seq_len]`)ï¼Œæ§åˆ¶æ³¨æ„åŠ›èŒƒå›´ |
| `position_ids`         | ä½ç½®ç¼–ç  IDï¼ˆè‹¥ `None`ï¼Œé»˜è®¤ä½¿ç”¨ `0,1,2,...`ï¼‰             |
| `output_hidden_states` | æ˜¯å¦è¿”å›æ‰€æœ‰ `hidden_states`                               |
| `output_attentions`    | æ˜¯å¦è¿”å› `attention` æƒé‡                                  |
| `use_cache`            | æ˜¯å¦è¿”å› `kv_cache`ï¼ˆç”¨äºæ¨ç†åŠ é€Ÿï¼‰                        |
| `past_key_value`       | ä¹‹å‰çš„ `key, value` ç¼“å­˜ï¼ˆç”¨äºæ¨ç†åŠ é€Ÿï¼‰                   |

------

### **1. æ£€æŸ¥ `input_ids`**

```python
if input_ids is None:
    raise ValueError("Please provide input_ids")
```

- ç¡®ä¿ `input_ids` ä¸ä¸ºç©ºï¼Œå¦åˆ™æŠ›å‡ºå¼‚å¸¸ã€‚

------

### **2. åˆå§‹åŒ–å­˜å‚¨å˜é‡**

```python
all_hidden_states = () if output_hidden_states else None
all_attentions = () if output_attentions else None
```

- `all_hidden_states`ï¼šå­˜å‚¨æ¯å±‚ `hidden_states`ï¼ˆè‹¥å¯ç”¨ï¼‰
- `all_attentions`ï¼šå­˜å‚¨ `attention_weights`ï¼ˆè‹¥å¯ç”¨ï¼‰

------

### **3. è¯åµŒå…¥**

```python
inputs_embeds = self.embed_tokens(input_ids)   # (bsz, seq_len, hidden_size)
hidden_states = inputs_embeds
```

- é€šè¿‡ `self.embed_tokens` è·å–è¯å‘é‡
- `hidden_states` åˆå§‹å€¼è®¾ä¸ºè¯å‘é‡

------

### **4. è¿›å…¥ Decoder å±‚**

```python
for idx, decoder_layer in enumerate(self.layers):
```

- éå† **æ¯ä¸€å±‚ Transformer è§£ç å™¨**
- `idx`ï¼šå½“å‰å±‚ç´¢å¼•
- `decoder_layer`ï¼šå½“å‰å±‚

------

### **5. ä¼ å…¥ Decoder å±‚**

```python
layer_outputs = decoder_layer(
    hidden_states,
    attention_mask=attention_mask,
    position_ids=position_ids,
    past_key_value=None if past_key_value is None else past_key_value[idx],
    output_attentions=output_attentions,
    use_cache=use_cache,
)
```

- è¿›å…¥ `decoder_layer`
  - `hidden_states` ä½œä¸ºè¾“å…¥
  - ä¼ é€’ `attention_mask`
  - å¤„ç† `past_key_value`ï¼ˆç”¨äº KV ç¼“å­˜ï¼‰
  - å¯èƒ½è¿”å› `attention_weights`
  - å¯èƒ½è¿”å› `present_key_value`

------

### **6. å½’ä¸€åŒ–å±‚**

```python
hidden_states = self.norm(hidden_states)
```

- `RMSNorm` **å½’ä¸€åŒ–æœ€ç»ˆè¾“å‡º**

------

### **7. ç»„ç»‡è¾“å‡º**

```python
outputs = (hidden_states,)
if output_hidden_states:
    outputs += (all_hidden_states,)
if output_attentions:
    outputs += (all_attentions,)
if use_cache:
    outputs += (present_key_value,)
```

- **é»˜è®¤è¿”å› `hidden_states`**
- **å¯èƒ½è¿”å› `hidden_states`ï¼ˆæ¯å±‚ï¼‰**
- **å¯èƒ½è¿”å› `attention_weights`**
- **å¯èƒ½è¿”å› `kv_cache`ï¼ˆæ¨ç†ç”¨ï¼‰

### **æ ¸å¿ƒç»“æ„**

âœ… **è¯åµŒå…¥ (`Embedding`)**
 âœ… **å¤šä¸ª Transformer è§£ç å±‚**
 âœ… **RMSNorm å½’ä¸€åŒ–**
 âœ… **æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹**
 âœ… **æ”¯æŒ `kv_cache`ï¼ˆæ¨ç†åŠ é€Ÿï¼‰**

------

### **ä»£ç æµç¨‹**

1. **è·å– `input_ids`**
2. **è¯åµŒå…¥ (`nn.Embedding`)**
3. **å¾ªç¯é€šè¿‡ `num_hidden_layers` ä¸ª `Qwen2DecoderLayer`**
4. **æœ€ç»ˆé€šè¿‡ `RMSNorm` å½’ä¸€åŒ–**
5. **è¿”å› `hidden_states`ã€`attention_weights`ã€`kv_cache`ï¼ˆå¯é€‰ï¼‰**

------

### **é€‚ç”¨åœºæ™¯**

âœ… **GPT è¯­è¨€æ¨¡å‹**
 âœ… **å¤§è§„æ¨¡æ–‡æœ¬ç”Ÿæˆï¼ˆLLMï¼‰**
 âœ… **æ¨ç†ä¼˜åŒ–ï¼ˆ`kv_cache`ï¼‰**
 âœ… **é•¿æ–‡æœ¬å»ºæ¨¡ï¼ˆ`attention_mask`ï¼‰**

è¿™åŸºæœ¬æ˜¯ **GPT-like æ¨¡å‹çš„æ ‡å‡†å®ç°**ï¼Œå¯ä»¥ç”¨äº **èŠå¤© AIã€ä»£ç ç”Ÿæˆã€æ–‡æœ¬è¡¥å…¨ç­‰ä»»åŠ¡ï¼** ğŸš€